---
title: "Predict which Brand of Products Customers Prefer"

author: "José Pedro Conceição"
date: "March 25, 2019"
output: 

  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(readr)
library(caret)
library(dplyr)
library(ggplot2)
library(tidyr)
library(scales)

```
```{r echo= FALSE}


CompleteResponsesOG <- read.csv(file = "/home/zordo/Downloads/CompleteResponses.csv" , header = TRUE , sep =",")

CompleteResponsesOG$brand <- factor(CompleteResponsesOG$brand,
                                    levels = c(0 ,1 ) ,
                                    label = c("Acer","Sony"))

CompleteResponsesOG$elevel <- factor(CompleteResponsesOG$elevel,
                                     levels = c(0,1,2,3,4),
                                     labels = c("No high school", "High School", "Some College", "College Degree", "Masters PHD"))
#CarBrand
CompleteResponsesOG$car <- factor(CompleteResponsesOG$car,
levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20),
  labels = c("BMW","Buick", "Cadillac", "Chevrolet","Chrysler","Dodge","Ford","Honda","Hyundai","Jeep","Kia","Lincoln","Mazda","Mercedes","Mitsubishi","Nissan","Ram","Subaru","Toyota","None"))
#Region
CompleteResponsesOG$zipcode <- factor(CompleteResponsesOG$zipcode,
                                      levels = c(0,1,2,3,4,5,6,7,8),
                                      labels = c("New England","Mid Atlantic","East North Central","West North Central","South Atlantic","East South Central","West South Central","Mountain","Pacific"))


IncompleteSurvery <- read.csv(file ="/home/zordo/Downloads/SurveyIncomplete.csv",header = TRUE , sep = ",")


IncompleteSurvery$brand <- factor(IncompleteSurvery$brand,
                                    levels = c(0 ,1 ) ,
                                    label = c("Acer","Sony")
)

IncompleteSurvery$elevel <- factor(IncompleteSurvery$elevel,
                                     levels = c(0,1,2,3,4),
                                     labels = c("No high school", "High School", "Some College", "College Degree", "Masters PHD"))
#CarBrand
IncompleteSurvery$car <- factor(IncompleteSurvery$car,
                                  levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20),
                                  labels = c("BMW","Buick", "Cadillac", "Chevrolet","Chrysler","Dodge","Ford","Honda","Hyundai","Jeep","Kia","Lincoln","Mazda","Mercedes","Mitsubishi","Nissan","Ram","Subaru","Toyota","None"))
#Region
IncompleteSurvery$zipcode <- factor(IncompleteSurvery$zipcode,
                                      levels = c(0,1,2,3,4,5,6,7,8),
                                      labels = c("New England","Mid Atlantic","East North Central","West North Central","South Atlantic","East South Central","West South Central","Mountain","Pacific"))


```
  
<br/>
<br/>
<br/>
  
##Executive summary 

<br/>
<br/>
<br/>

```{r echo=FALSE}
BrandAgendSalary <- ggplot(CompleteResponsesOG, aes(x=salary, y=age)) + 
  geom_point(aes(col=brand), size=3) +  # Set color to vary based on state categories.
 
  labs(title="Brand by Age and Salary", subtitle="", y="Age", x="Salary")
plot(BrandAgendSalary)
 
BarBrandPlot <- ggplot(CompleteResponsesOG, aes(brand)) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels=scales::percent) +
  ylab("Percentage of Volume")
```
 <br/>
<br/>


  This is the most valueble information provided by the data, this type of data is excellent for customer profilling, as shown it can establish relationship between age, salary and brand.
  However the way the data was collected will not help us providing new insights(with the IncSurvey filled with predictions), in making a decision in which company (Sony or Acer) should we invest in the pursue of a better relationship
   This happens because the data is not a random distribution, it is actuallly  homogeneous  distributed, in "real life" we would have some low salaries alot of middle value salaries and perhaps a few high salaries, but very very rarely would we have a distribution like this.
   This happens for both Surveys :
  
  
```{r echo = FALSE}

HistogramOfSalary <- qplot(CompleteResponsesOG$salary,
                           geom="histogram",
                           main = "Histogram of Salaries",
                           xlab = "Salary" ,
                           fill=I("blue"),
                            col=I("red"),
                            alpha=I(.2))
HistogramOfSalary


HistogramOfAge <- qplot(CompleteResponsesOG$age,
      geom="histogram",
        
      main = "Histogram for Age", 
      xlab = "Age",  
      fill=I("blue"), 
      col=I("red"), 
      alpha=I(.2))

HistogramOfAge

BarCarPlot <- ggplot(CompleteResponsesOG, aes(car)) + 
  geom_bar(aes(y = ..count..))  

BarCarPlot

```
<br/>
<br/>
<br/>
  
  If we want to answer the question that was proposed to us, we need to get data that translates into a real world scenario, it is not possible to add any valueble decision making, since the algorithm will just mimmick the CompleteSurvey Results on to the Incomplete Survey, and give the exact same porpotion of Acer/Sony in percentage. And if summed together the porpotion stays the same.(More detail in the Technical section)
  Note: I only chose age , salary and age because they are the most important attributes in the context of the problem, it is explained why in the technical section.

```{r echo= FALSE}
BarBrandPlot <- ggplot(CompleteResponsesOG, aes(brand)) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels=scales::percent) +
  ylab("Percentage of Volume")
  BarBrandPlot
```





## Technical Section 

<br/>
<br/>

This sections is meant for more people with some knowdlge on data science, more specifically on machine learning and R.
<
br/>
<br/>


### Important Variables

<br/>()
<br/>
<br/>
<br/>

By using a C5.0 model we can assess the importance of each variable in percentage with the varImp() function  

<br/>
<br/>

```{r echo=FALSE }

TreeAll <- load("CompleTree.rsa")

```

```{r }

varImp(TreeComplete)

```
<br/>
<br/>
<br/>



###Training and Model Selection  
<br/>
<br/>
<br/>


####Decision Tree C5.0

<br/>
<br/>
<br/>


##### 1st model

<br/>
<br/>
<br/>

I started with a decision tree C5.0, and this is the code for the training indices and traincontrol :

``` {r }

fitControl <- trainControl(method = "repeatedcv", repeats = 10)
inTrain <- createDataPartition(y = CompleteResponsesOG$brand , p = 0.75, list = FALSE )
training <- CompleteResponsesOG[inTrain,]
testing <- CompleteResponsesOG[-inTrain,]

```

<br/>
<br/>
<br/>

Here we begin training our tree, first we use all features to see which of them are important.

<br/>
<br/>
``` {r }
#TreeComplete <- train(brand ~.,
  #                    data = training, 
  #                    method = "C5.0",
   #                   trcontrol = fitControl,
    #                  tuneLength = 2 
      #    ) 

save(TreeComplete, file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/CompleTree.rsa")

TreeComplete # shows kappa and acc on training set 


BrandPredictionTree <- predict(TreeComplete, newdata = testing)


postResample(BrandPredictionTree, testing$brand )

```
<br/>
<br/>
Using postResample function we can see our test metrics, and see if our model is overfitting or not, as you can confirm the accuracy and kappa for both training and test sets (CompleteSurvey) are very similar, this translates into a good, non-overfitted model.

<br/>
<br/>
<br/>

After using varImp, I chose  Salary Age and Car for my next C5.0 model.

<br/>
<br/>
<br/>

##### Final Model

<br/>
<br/>
<br/>

``` {r}

#TreeASC <- train(brand ~ salary + age + car ,
 #                 data = training,
  #                method = "C5.0",
   #               trcontrol = fitControl,
    #              tuneLenght = 2
     #            )

##save(TreeASC, file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/TreeASC.rsa")

load(file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/TreeASC.rsa")

BrandPredictionTreeASC <- predict(TreeASC, newdata = testing)

TreeASC

confusionMatrix(BrandPredictionTreeASC,testing$brand)

postResample(BrandPredictionTreeASC, testing$brand )

```
<br/>
<br/>


```{r echo=FALSE}

Predictions <- predict(TreeASC, testing)

Testing2 <- cbind(testing, Predictions)
Error <- Predictions != testing$brand
Testing3 <- cbind(Testing2, Error)

ErrorPlot <- ggplot(Testing3, aes(x=salary, y=age)) + 
  geom_point(aes(col=Error), size=2) +  # Set color to vary based on state categories.
  
  labs(title="Model Error Points", subtitle="", y="Age", x="Salary")
plot(ErrorPlot)

```

  This scatter plot shows the cases where our model didn't predict correctly, this is interesting, and it  makes perfect sense that our model failures are on the borders of our previous Age / Salary / brand scatterplot, because the border of every "bin", is where the algorithm will struggle to make a correct prediction.
  
  
  
```{r echo = FALSE}
BrandAgendSalary <- ggplot(CompleteResponsesOG, aes(x=salary, y=age)) + 
  geom_point(aes(col=brand), size=3) +  # Set color to vary based on state categories.
 
  labs(title="Brand by Age and Salary", subtitle="", y="Age", x="Salary")
plot(BrandAgendSalary)
```
  
<br/>

<br/>
  Our results barely improved, but as always it's faster to process less variables then all of them. 
  In the task it was asked to use a Random Forest, and that is why I am going to use it, however I don't think that increasing our accuracy and kappa  by a few percentage will help with the fundamental issue of this task.
  
<br/>
<br/>
<br/>

####Random Forest

<br/>
<br/>
<br/>


##### First Model

<br/>
<br/>
<br/>


  Random Forest Algorithm with salary, age and car.

<br/>
<br/>
<br/>
```{r}
# RandomForestwCar <- train(brand ~ salary + age + car,
#                       data = training,
#                       method="rf",
#                       trcontrol = fitControl,
#                       tunegrid = expand.grid(mtry= c(1,2,3,4,5)) )
# 
# save(RandomForestwCar, file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/RandomForestwCar.rsa")

load(file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/RandomForestwCar.rsa")
varImp(RandomForestwCar)

BrandPredictionRandomForestwCar <- predict(RandomForestwCar, newdata = testing)
RandomForestwCar

confusionMatrix(BrandPredictionRandomForestwCar,testing$brand)

postResample(BrandPredictionRandomForestwCar,testing$brand)
```
<br/>
<br/>
This are some excellent results on the  Test-Set,let's remove cars, since it is not an important variable and see what happens.
<br/>
<br/>
<br/>

##### Final Model 

<br/>
<br/>
<br/>


```{r}
# RandomForest <- train(brand ~ salary + age  ,
#                             data = training,
#                             method="rf",
#                             trcontrol = fitControl,
#                             tunegrid = expand.grid(mtry=c(1,2,3,4,5)) )
# 
# 
# 
# save(RandomForest, file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/RandomForest.rsa")
load(file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/RandomForest.rsa")
varImp(RandomForest)
BrandPredictionRF <- predict(RandomForest, newdata = testing)

RandomForest

confusionMatrix(data = BrandPredictionRF, testing$brand)

postResample(BrandPredictionRF,testing$brand)
```

```{r echo = FALSE}


Predictions <- predict(RandomForest, testing)

Testing2 <- cbind(testing, Predictions)
Error <- Predictions != testing$brand
Testing3 <- cbind(Testing2, Error)

ErrorPlot <- ggplot(Testing3, aes(x=salary, y=age)) + 
  geom_point(aes(col=Error), size=2) +  # Set color to vary based on state categories.
  
  labs(title="Model Error Points", subtitle="", y="Age", x="Salary")
plot(ErrorPlot)



```

<br/>
<br/>
<br/>

Alot less errors ! But the border can still be seen clearly ! Let's try with another algorithm

<br/>
<br/>


<br/>
<br/>

#### K-NN

<br/>
<br/>

Unlike other models, K-NN requires a little of detailed atention. Why ? Because the way the model fundamentally works, having different scaling features will cause issues, and this is why we need to normalize the data.


```{r echo }


normalize2 <- preProcess(CompleteResponsesOG[,], method=c("range"))
normalize2
Complete_norm <- predict(normalize2, CompleteResponsesOG)
set.seed(28)
inTraining_norm<-createDataPartition(Complete_norm$brand, p=.75, list = FALSE)
training_norm<-Complete_norm[inTraining_norm,]
testing_norm<-Complete_norm[-inTraining_norm,]



knnFitAS <- train(brand ~ age + salary, training_norm, method = "knn", trControl=fitControl)

save(knnFitAS, file = "/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/knnfit.rsa")

load(file="/home/zordo/Documents/Ubiqum/R/RTask2/Project/Task2/Task2AllOver/knnfit.rsa")

varImp(knnFitAS)
BrandPredictionKNN <- predict(knnFitAS, newdata = testing_norm )

knnFitAS

confusionMatrix(data = BrandPredictionKNN, testing_norm$brand)

postResample(BrandPredictionKNN,testing_norm$brand)
```

<br/>
<br/>
 
 Let's plot the error like we did for the other models and see what happens ! 
 
 
<br/>
<br/>
```{r echo=FALSE}



Testing2 <- cbind(testing_norm, BrandPredictionKNN)
Error <- BrandPredictionKNN != testing_norm$brand
Testing3 <- cbind(Testing2, Error)

ErrorPlot <- ggplot(Testing3, aes(x=salary, y=age)) + 
  geom_point(aes(col=Error), size=2) +  # Set color to vary based on state categories.
  
  labs(title="Model Error Points", subtitle="", y="Age", x="Salary")
plot(ErrorPlot)


```


 
###Incomplete-Survey 

<br/>
<br/>
```{r}
IncompleteSurvery <- read.csv(file ="/home/zordo/Downloads/SurveyIncomplete.csv",header = TRUE , sep = ",")
```



```{r echo=FALSE}



IncompleteSurvery$brand <- factor(IncompleteSurvery$brand,
                                    levels = c(0 ,1 ) ,
                                    label = c("Acer","Sony")
)

IncompleteSurvery$elevel <- factor(IncompleteSurvery$elevel,
                                     levels = c(0,1,2,3,4),
                                     labels = c("No high school", "High School", "Some College", "College Degree", "Masters PHD"))
#CarBrand
IncompleteSurvery$car <- factor(IncompleteSurvery$car,
                                  levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20),
                                  labels = c("BMW","Buick", "Cadillac", "Chevrolet","Chrysler","Dodge","Ford","Honda","Hyundai","Jeep","Kia","Lincoln","Mazda","Mercedes","Mitsubishi","Nissan","Ram","Subaru","Toyota","None"))
#Region
IncompleteSurvery$zipcode <- factor(IncompleteSurvery$zipcode,
                                      levels = c(0,1,2,3,4,5,6,7,8),
                                      labels = c("New England","Mid Atlantic","East North Central","West North Central","South Atlantic","East South Central","West South Central","Mountain","Pacific"))


```
<br/>
<br/>
Since we already saved the models  we don't need to rewrite them.
<br/>
<br/>
```{r}

predictionTreeALLincResp <- predict(RandomForest , newdata = IncompleteSurvery)


IncompleteSurvery[8] <- NULL ## to delete the full 0's column on the IncompleteSurveys dataframe

IncompleteSurvery <- cbind(IncompleteSurvery, predictionTreeALLincResp)
#bind the new predictions on the Survey.
```

<br/>
<br/>
<br/>

  We cannot use postResample, because we are  comparing  data(column) that does not exist, since we are trying to predict the brand of the IncompleteSurvey, the reason why some low values appear is because the IncompleteSurvey is saved with all 0's and that translates to a specific brand. You can not see the test results, because there is nothing to compare them to.

<br/>
<br/>
<br/>

  Let's plot again the Age / Salary with brand , and see if in fact our model mimmicked the CompleteSurvey.
 
  <br/>
<br/>
<br/>

```{r echo=FALSE}

predictionRandomForest <- predict(RandomForest , newdata = IncompleteSurvery)



BrandAgendSalaryInc <- ggplot(IncompleteSurvery, aes(x=salary, y=age)) + 

  geom_point(aes(col=predictionRandomForest), size=3) +
  labs(title="Brand by Age and Salary", subtitle="", y="Age", x="Salary")


BrandAgendSalaryInc




```
<br/>
<br/>
<br/>
  Comparing with the previous one we can see the smiliraties.(same pattern)
  
  
```{r echo=FALSE}

plot(BrandAgendSalary)

```
  
<br/>
<br/>
  The latter having more density for the sole fact that it has more observations.
 <br/>
<br/> 

### Why does this happen ? 

<br/>
<br/>
<br/>
<br/>

This happens because our data is not normally distributed, it's homogeneous distributed, which for our particular problem translates into an impossible task.
  We can prove that by ploting a some qqplots:
  <br/>
<br/>
<br/>
<br/>



```{r}
qqnorm(CompleteResponsesOG$salary)
qqnorm(CompleteResponsesOG$age)
qqnorm(CompleteResponsesOG$credit)



```

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

This simple image explains how different data distributions have effect on the qqplot. 
<br/>
<br/>
<br/>


<img src="https://bioinfo.iric.ca/wpbioinfo/wp-content/uploads/2015/10/qqplot.png" alt="https://bioinfo.iric.ca/wpbioinfo/wp-content/uploads/2015/10/qqplot.png" class="shrinkToFit" width="486" height="389"> 


<br/>
<br/>
<br/>
 If we wanted to decide which company brand to pursue we would need a normal distribution to have valued results on the Incomplete Survey ! 
<br/>
<br/>
<br/>
<br/>
For the Incomplete Survey:
<br/>
<br/>

```{r}
qqnorm(IncompleteSurvery$salary)
qqnorm(IncompleteSurvery$age)
qqnorm(IncompleteSurvery$credit)

```



## Conclusion
<br/>
<br/>
So if we want to make customer profiles we want data that is homogeneous distributed, and if we want to make predictions we want it as "real" distributed as possible
  
<br/>
<br/>
<br/>
<br/>
<br/>

## Extras
<br/>
<br/>
On this section I included some functions I created in R that facilitates creating sets, training models and predicting.
<br/>
<br/>
```{r}
#### Pre-processing function ####




pProcessing <- function(data)
{
  
  
  
  
  data$brand <- factor(data$brand,
                                      levels = c(0 ,1 ) ,
                                      label = c("Acer","Sony"))
  
  data$elevel <- factor(data$elevel,
                                       levels = c(0,1,2,3,4),
                                       labels = c("No high school", "High School", "Some College", "College Degree", "Masters PHD"))
  #CarBrand
  data$car <- factor(data$car,
                                    levels = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20),
                                    labels = c("BMW","Buick", "Cadillac", "Chevrolet","Chrysler","Dodge","Ford","Honda","Hyundai","Jeep","Kia","Lincoln","Mazda","Mercedes","Mitsubishi","Nissan","Ram","Subaru","Toyota","None"))
  #Region
  data$zipcode <- factor(data$zipcode,
                                        levels = c(0,1,2,3,4,5,6,7,8),
                                        labels = c("New England","Mid Atlantic","East North Central","West North Central","South Atlantic","East South Central","West South Central","Mountain","Pacific"))
  
  
 
  
}



#### Training/Testing Set Function ####

TrainAndTestSets <- function(label,p,data){
  
    inTrain <- createDataPartition(y= label, p = p , list = FALSE)
                                training <- data[inTrain]
                                testing <- data[-inTrain]
                                
                  list(trainingSet=training ,testingSet=testing )              
  
  
}

#### Training Model Functions #### 

TrainModel <- function(formula,data,method,tunelength = 0,tunegrid = 0)
{
  fitControl <- trainControl(method = "repeatedcv", repeats = 10)
  if (method == "rf")
  {
    model <- train(formula , 
                   data = data ,
                   method = method ,
                   trcontrol =fitControl,
                   tunegrid = tunegrid  )
  }
     else 
    {
      model <-train(formula ,
                    data = data ,
                    method = method,
                    trcontrol= fitControl,
                    tunelength = tunelength)
    }
    
  model
  }



#### Predicting Function and postResample ####


PredictionsFunction <- function(model,newdata,label)
{
  Brandpredictions <- predict(model,newdata =newdata)
  
  
  Brandpredictions
}


```




  

  
  