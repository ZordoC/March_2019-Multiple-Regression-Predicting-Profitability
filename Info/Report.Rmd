
---
title: "Multiple Regression "

author: "José Pedro Conceição,Kiko Sánchez , Eloi Cirera"
date: "March 25, 2019"
output: 

  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    theme: "cerulean" 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(readr)
library(rstudioapi)
library(e1071)
library(dplyr)
library(rpart)
library(reshape)
library(corrplot)
#library(prettydoc)
#
#

#For cleaning variables
#rm(list = setdiff(ls(), lsf.str()))

#current_path = getActiveDocumentContext()$path
#setwd(dirname(current_path))
#setwd("..")
#rm(current_path)
EP <- read.csv( file ="/home/zordo/Documents/Ubiqum/R-Task3/data/Epa.csv" , header = TRUE , sep = ',')
NP <- read.csv(file = "/home/zordo/Documents/Ubiqum/R-Task3/data/Npa.csv", header = TRUE , sep =',')


```
</br>
</br>
</br>
</br>

## Executive Summary

</br>
</br>
</br>
</br>



  The first Objective was accurately Predicting Sales Volume, this has not been fully achieved because of the small data set provided to the team, we did train some models and made some predictions,however, they accurate and the errors are big, but it was the best information value we could extract from this sample.
</br>
    Predicting the sales of new Products using a reduced sample won't be accurate, besides being statistically unsound.
</br>
  We found out that what actually predicts success in volume are both, however the best predictor for successe comes from service reviews with an importance of 100% according to a random forest algorithm, followed by a a 50% importance of 4 star reviews.  
</br> 
   Why 4 star reviews and not other reviews ? Well because they had to be taken out of our model training, they had levels of relationship with our predictor (Volume) so high that they were biasing the whole model, making the predictions even more unreliable.For example the 5 star review had a perfect correlation with the Volume, this means that the volume would grow at the same rate as a the 5 star reviews increased, which does not translate into reality.
</br>
</br>
Nonetheless here are our final predictions.
</br>
</br>
</br>
</br>
</br>



| Number       |  Volume       |  Profit |
|--------------|:-------------:|--------:|
| Game Console |  1735.3872    |347.07744|
| Game Console |  1735.3872    |312.36970|
| Tablet       |  1319.6391    |118.76752|
| Tablet       |  1211.3776    |109.02398|
| NoteBook     |  877.6156     |87.76156 |


</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

## Technical Report 

</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

### Pre-process

</br>
</br>
</br>
</br>
  
  We always start by assesing the importance of each variable, we achieve this by doing a correlation matrix and training a simple model, followed by a varImp(), which will give us in percentage the importance of the variable for the model's prediction.(We need to first use a correlation matrix to see the correlation values, and take out anything that migh bias our model, otherwise the "biased", features will just appear at the top of the varImp() output).   
</br>
</br>
</br>
  We created a function to dummyfy the variables and to check if there was any NA values (in any attribute), and if they exist, remove them, I also included  a function that removes outliers, and a function to subset the data into different product types.
</br>
</br>
</br>

### Process functions 
</br>
</br>
</br>
</br>

#### Pre-process
</br>
</br>


```{r}
PPfunction <- function(data) {
  
  N <- dummyVars(" ~ .", data = data)
  
  N <- data.frame(predict(N, newdata = data))
  
  N <- N[,colSums(is.na(N)) == 0] 
  
  N 
}
```
</br>
</br>
</br>
</br>

#### Remove Outliers


</br>
</br>
```{r} 
RmOut <- function(D,V)

  {
  
  Out <- boxplot(D$V ,plot = FALSE)$out
  K <- D[-which(D$V %in% Out),]
  K
  
}

```
</br>
</br>
</br>

#### Sub-set by product types 
</br>
</br>
</br>
```{r}
SubSetDataProductTypes <- function(data,p,p1 = 0,p2 = 0 , p3 = 0 , p4 = 0)

  {
  if ( p1 == 0 && p2 == 0 && p3 == 0 && p4 == 0)
  {
    Nsub <- subset(data, data$ProductType == p)
    
    return(Nsub)
  }
  else if (p2 == 0 && p3 == 0 && p4 == 0){
    Nsub <- subset(data, data$ProductType == p)
    
    Nsub2 <-subset(data, data$ProductType == p1)
    
    Nsub2 <- rbind(Nsub,Nsub2)
    
    return(Nsub2)
  }
  else  if (p3 == 0 && p4 == 0) 
  {
    
    Nsub <- subset(data, data$ProductType == p)
    
    Nsub2 <-subset(data, data$ProductType == p1)
    
    Nsub3 <- subset(data,data$ProductType == p2)
    
    Nsub3 <- rbind(Nsub,Nsub2,Nsub3)
    
    return(Nsub3)
    
  }
  
  else  if (p4 == 0){
    Nsub <- subset(data, data$ProductType == p)
    
    Nsub2 <-subset(data, data$ProductType == p1)
    
    Nsub3 <- subset(data,data$ProductType == p2)
    
    Nsub4 <- subeset(data,data$ProductType == p3)
    
    Nsub4 <- rbind(Nsub,Nsub2,Nsub3,Nsub4)
    
    return(Nsub4)
  }
  
  else{
    
    Nsub <- subset(data, data$ProductType == p)
    
    Nsub2 <-subset(data, data$ProductType == p1)
    
    Nsub3 <- subset(data,data$ProductType == p2)
    
    Nsub4 <- subeset(data,data$ProductType == p3)
    
    Nsub5 <- subset(data,data$ProductType == p4)
    
    Nsub5 <- rbind(Nsub,Nsub2,Nsub3,Nsub4,Nsub5)
    
    return(Nsub5)
  } 
}

#### I know it's not the most pretty or effective way to do this, but it works.



```
</br>
</br>
</br>

###  Correlation Matrix : 

</br>  
</br>  

```{r}
EP <- PPfunction(EP)
EP <- RmOut(EP,Volume)

corr_all<-cor(EP)



corrplot:: corrplot(corr_all,type="upper",tl.pos="td",method="circle",tl.cex = 0.5,tl.col='black',diag=FALSE)
```
  </br>
  </br>
  </br>
  </br>
  </br>
  
  This information does not differ from the module 1's counterpart, it's obvius because we are using the same data set.
  
  </br>  
  </br>
  </br>
  
  We trained a random forest followed by the use of varImp() function that assess the importance of each variables (without the ones we took out by looking at the correlation matrix).
  But first we need to create Test and Training sets, we also came up with a simple function to automate the processs.
  </br>
  </br>
  </br>
  </br>
  </br>
  
#### Train and Test Set function  
  </br>
  </br>
  </br>
  </br>
  
```{r}

TrainAndTestSets <- function(label,p,data,seed){
  set.seed(seed)
  
  inTrain <- createDataPartition(y= label, p = p , list = FALSE)
  training <- data[inTrain,]
  testing <- data[-inTrain,]
  
  
  list(trainingSet=training,testingSet = testing)
  
}

```
</br>
</br>
</br>
</br>
```{r}

EP <- EP[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,18,20,21,22,23,24,25,26,27,28)]

List <- TrainAndTestSets(EP$Volume,0.75,EP,123)
```
</br>
</br>
</br>


```{r, warning=FALSE}

 fitcontrol <-  trainControl(method = "repeatedcv", repeats = 4)

  Model <- train(Volume~., data = EP,method = "rf", trcontrol = fitcontrol , tunelenght = 5
                 , preProcess = c("center", "scale"),importance=T)  
  
  varImp(Model)
  
  
  
  


```

</br>
</br>
</br>
</br>

  So the only variables with a significant impact are only PostiveServiceReview and x4StarReviews.
  
</br>
</br>
  
  
  
## Models and Predictions 

</br>
</br>
</br>
</br>

### Training Function 
  
</br>
</br>
</br>
  I created a function that trains every different model, by user specification
</br>
</br>
</br>

```{r, warning=FALSE}

TrainingFunction <- function(method,formula,data,tune = 0,c=1000,gamma=0.0001)
  {
  
  fitcontrol <-  trainControl(method = "repeatedcv", repeats = 4)
  
  if(method == "rf") {
    
    Model <- train(formula, data = data,method = method, trcontrol = fitcontrol , tunelenght = tune)  
  }

  else if (method == "knn"){
    
    
    
    Model <- train(formula, data = data,method = method, trcontrol = fitcontrol , tunelenght = tune,
                   preProcess = c("center", "scale"))  
  
    
    }
  
   
  else if (method == "svm"){ 
     
    Model <- svm(formula, data = data,cost=c , gamma = gamma)
  
    
    }

    return(Model)
  }


```

</br>
</br>
</br>


```{r, warning=FALSE}



EP <- read.csv( file ="/home/zordo/Documents/Ubiqum/R-Task3/data/Epa.csv" , header = TRUE , sep = ',')

EP <- EP[,c(1,5,9,18)]

EP <- PPfunction(EP)

EP <- RmOut(EP)

List <- TrainAndTestSets(EP$Volume,0.75,EP,123)


#### Random Forest ####
ModelRandomForest <- TrainingFunction("rf",Volume~.,List$trainingSet,5)

PredictionRandomForest <- predict(ModelRandomForest,List$testingSet)

TestResultsRF <- postResample(PredictionRandomForest,List$testingSet$Volume)

#### SVM ####

svm.model <- TrainingFunction("svm",Volume~.,List$trainingSet,5,10000000,0.0000001)  

svm.pred <- predict(svm.model,List$testingSet)

TestResultsSVM <- postResample(svm.pred,List$testingSet$Volume)


#### knn ####

 KNN <- TrainingFunction("knn",Volume~.,List$trainingSet,30)

KnnPrediction <- predict(KNN,List$testingSet)

TestResultsKNN <-postResample(KnnPrediction,List$testingSet$Volume)

####


AllTestResults <- cbind(TestResultsKNN,TestResultsRF,TestResultsSVM)

AllTestResults    


```
</br>
</br>
</br>
</br>

  And then did another one t train the three models at the same time with a for 
loop

</br>
</br>
</br>
</br>
```{r,warning=FALSE}
TrainAll3Models <- function (formula,data)
  {

  Model <- vector(mode="list", length=length(methods))

      methods <- c("rf","svm","knn")
        
      for(i in 1:length(methods))
          {    
      
            Model[[i]] <- TrainingFunction(methods[i],formula,data,5)

               

      }
    Model
}

```
</br>
</br>
</br>
  I didn't   use this function that much since the mentors showed us another way of training without any function, and it's much easier and cleaner.
</br>
</br>
</br>
```{r,warning=FALSE}

a <- c("Volume ~ x4StarReviews","Volume ~.","Volume ~ PositiveServiceReview")
b <- c("lm","rf", "knn","svmLinear")
compare_var_mod <- c()

for ( i   in a) {
  for (j in b) {
    
    model <- train(formula(i), data = List$trainingSet, method = b,trainControl=trainControl(method = "repeatedcv", repeats = 4))
    
    pred <- predict(model, newdata = List$testingSet)
    
    pred_metric <- postResample(List$testingSet$Volume, pred)
    
    compare_var_mod <- cbind(compare_var_mod , pred_metric)
    
  }
  
}
      compare_var_mod

names_var <- c()
for (i in a) {
  for(j in b) {
    names_var <- append(names_var,paste(i,j))
  }
}


names_var


colnames(compare_var_mod) <- names_var

compare_var_mod


compare_var_mod_melt <- melt(compare_var_mod, varnames = c("metric", "model"))
compare_var_mod_melt <- as.data.frame(compare_var_mod_melt)
compare_var_mod_melt



ggplot(compare_var_mod_melt, aes(x=model,y=value)) + geom_col() + facet_grid(metric~., scales="free") +theme(axis.text=element_text(size=3),
        axis.title=element_text(size=14,face="bold"))

```
</br>
</br>
</br>
  I only used RF, and KNN because from past results the SVM did not look like a good fit.
</br>
</br>

###Error Analysis 


```{r}


ABSrf <- (List$testingSet$Volume - PredictionRandomForest)

RLTrf <-  (ABSrf / List$testingSet$Volume)

ABSsvm <- (List$testingSet$Volume - svm.pred)

RLTsvm <- (ABSsvm / List$testingSet$Volume)



Absknn <- (List$testingSet$Volume - KnnPrediction)

RLTknn <-  (Absknn / List$testingSet$Volume)
 #abline(0, 0)                  # the horizon


Lol <- cbind(List$testingSet,ABSrf)

```
</br>
</br>
Random Forest Residuals 
</br>
</br>
```{r}

ggplot(Lol,
      aes(Lol$Volume,ABSrf))+
 geom_point(color="red")+
 geom_smooth()


ggplot(Lol,
      aes(Lol$Volume,RLTrf))+
 geom_point(color="red")+
 geom_smooth()

```
</br>
</br>
Svm Residuals
</br>
</br>
```{r}

ggplot(Lol,
      aes(Lol$Volume,ABSsvm))+
 geom_point(color="red")+
 geom_smooth()


ggplot(Lol,
      aes(Lol$Volume,RLTsvm))+
 geom_point(color="red")+
 geom_smooth()


```
</br>
</br>
</br>
</br>
```{r}
ggplot(Lol,
      aes(Lol$Volume,Absknn))+
 geom_point(color="red")+
 geom_smooth()


ggplot(Lol,
      aes(Lol$Volume,RLTknn))+
 geom_point(color="red")+
 geom_smooth()
```


</br>
</br>
  Let's now apply the current models into the new product list and make a top 5 for most probably sold products in volume.
</br>
</br>
</br>

### Prediction 

</br>
</br>
</br>
</br>
  The random forest was the one who gave us the best results, with both variables (ProductServiceReview,x4StarReviews).
</br>
</br>  
```{r}
NP <- read.csv(file = "/home/zordo/Documents/Ubiqum/R-Task3/data/Npa.csv", header = TRUE , sep =',')

NP <- PPfunction(NP)

NewProductsVolume <- predict(ModelRandomForest,NP)

NP$Volume<-NewProductsVolume


```
</br>
</br>  
</br>
</br>  
  We got the volume, now we need to calculate the profit, to see which products types we should invest on. 
</br>
</br>   

   **Profit = profit margin * Volume** 
    
</br>
</br>   
</br>
</br>   

```{r}
  
  Profit <- NP$ProfitMargin * NP$Volume

  NP <- cbind(NP,Profit)  

  
Top5 <-  top_n(NP, 5, Profit)
  
Top5 <- cbind (Top5,sort(Top5$Profit))
 
Top5

```
</br>
</br>  
</br>
Our top 5 most profitable product types are 
</br>  
</br>
</br>  
</br>

| Number       |  Volume       |  Profit |
|--------------|:-------------:|--------:|
| Game Console |  1735.3872    |347.07744|
| Game Console |  1735.3872    |312.36970|
| Tablet       |  1319.6391    |118.76752|
| Tablet       |  1211.3776    |109.02398|
| NoteBook     |  877.6156     |87.76156 |

</br>  
</br>
</br>  
</br>
</br>      

##Conclusion
</br>
</br>  
</br>
</br>  

  All three models used  are non-parametric, but before explaining what a non-parametric model is, I would like to explain what parametric models are.
</br>
  Parametric models are algorithms that simplify the function to a known form, and no matter how much data that's fed to the algorithm, the model won't change the quantity of parameters needed.All you need to know for predicting a future data balue from the current state of the model are his parameters,EG : Linear regression with on variable, you have two parameters (coefficient and intercept).Knowing this parameters will enable you to predict new values.
  In a mathematical way : 
</br>
</br>  
**Yi = B0 + B1X1 + B2X2 + ... + ei**
</br>
</br>
  Non-parametric models do not make any fixed assumptions about the form of the mapping, they are free to learn from the training data. The parameters are usually said to be infinite in dimension and so can express the characteristics in data much better than parametric models.
</br>
  E.g : KNN, makes predictions based on the k most similar training patterns for a new data instance. The method does not assume anything about the form of the mapping function other than patterns that are close are likely have a similar output variable.
In mathematical form
</br>
</br>
 **Yi=F(xi) + ei**
</br>
</br>
Where F can be any function,the data will decide what the function looks like.It will not provide the analytical expression but it will give you its graph given your data set.
</br>
</br>
<img src="https://qph.fs.quoracdn.net/main-qimg-f0ce4358dddb36da3d6d4fdad4b5dfc4" alt="https://qph.fs.quoracdn.net/main-qimg-f0ce4358dddb36da3d6d4fdad4b5dfc4" class="shrinkToFit" width="241" height="388">
</br>
</br>
</br>
</br>
I find this picture very helpfull in understanding how both types of models work, summing up in easy non- technical language, parametric models follow an equation while non-parametric models follow the data.
</br>
</br>
</br>
</br>
One of the requirements to use a non-parametric model is to have a big set of data, we have 80 rows, 78 after cutting outliers and only 60 of them are for training.This is not a big set of data at all, I think it actually couldn't be any smaller than this.
</br>
Also they are good methods when we don't have any prior knowledge and not worry about choosing the right features.
So using this models is not efficient at all, if we are using algorithms that "follow the data" and we have almost no data, it's obvious why this approach is  not efficient and why the errors are so big. 




